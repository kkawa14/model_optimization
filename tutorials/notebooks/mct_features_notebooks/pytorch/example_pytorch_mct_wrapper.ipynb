{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4a91956",
   "metadata": {},
   "source": [
    "# Model Compression Toolkit (MCT) Wrapper API Comprehensive Quantization Comparison (pytorch)\n",
    "\n",
    "[Run this tutorial in Google Colab](https://colab.research.google.com/github/SonySemiconductorSolutions/mct-model-optimization/blob/main/tutorials/notebooks/mct_features_notebooks/pytorch/example_pytorch_mct_wrapper.ipynb)\n",
    "\n",
    "## Overview \n",
    "This notebook provides a comprehensive demonstration of the MCT (Model Compression Toolkit) Wrapper API functionality, showcasing five different quantization methods on a MobileNetV2 model. The tutorial systematically compares the implementation, performance characteristics, and accuracy trade-offs of each quantization approach: PTQ (Post-Training Quantization), PTQ with Mixed Precision, GPTQ (Gradient-based PTQ), GPTQ with Mixed Precision. Each method utilizes the unified MCTWrapper interface for consistent implementation and comparison.\n",
    "\n",
    "## Summary\n",
    "1. **Environment Setup**: Import required libraries and configure MCT with MobileNetV2 model\n",
    "2. **Dataset Preparation**: Load and prepare ImageNet validation dataset with representative data generation\n",
    "3. **PTQ Implementation**: Execute basic Post-Training Quantization with 8-bit precision and bias correction\n",
    "4. **PTQ + Mixed Precision**: Apply intelligent bit-width allocation based on layer sensitivity analysis (75% compression ratio)\n",
    "5. **GPTQ Implementation**: Perform gradient-based optimization with 5-epoch fine-tuning for enhanced accuracy\n",
    "6. **GPTQ + Mixed Precision**: Combine gradient optimization with mixed precision for optimal accuracy-compression trade-off\n",
    "7. **Performance Evaluation**: Comprehensive accuracy assessment and comparison across all quantization methods\n",
    "8. **Results Analysis**: Compare model sizes, inference accuracy, and quantization trade-offs\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852cf568",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install onnx==1.16.1\n",
    "!pip install -q torch==2.6.0 torchvision==0.21.0\n",
    "!pip install tqdm\n",
    "from typing import Tuple, Callable\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0b00c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "if not importlib.util.find_spec('model_compression_toolkit'):\n",
    "    !pip install model_compression_toolkit\n",
    "\n",
    "# Import Model Compression Toolkit (MCT) core functionality for PyTorch\n",
    "import model_compression_toolkit as mct\n",
    "from model_compression_toolkit.core import QuantizationErrorMethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9da27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
    "from torchvision.datasets import ImageNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3ca588",
   "metadata": {},
   "source": [
    "Load a pre-trained MobileNetV2 model from torchvision, in 32-bits floating-point precision format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1d1be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = MobileNet_V2_Weights.IMAGENET1K_V2\n",
    "\n",
    "float_model = mobilenet_v2(weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0b43e6",
   "metadata": {},
   "source": [
    "## Dataset preparation\n",
    "### Download ImageNet validation set\n",
    "Download ImageNet dataset with only the validation split.\n",
    "\n",
    "**Note** that for demonstration purposes we use the validation set for the model quantization routines. Usually, a subset of the training dataset is used, but loading it is a heavy procedure that is unnecessary for the sake of this demonstration.\n",
    "\n",
    "This step may take several minutes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9566bb3e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.isdir('imagenet'):\n",
    "    !mkdir imagenet\n",
    "    !wget -P imagenet https://image-net.org/data/ILSVRC/2012/ILSVRC2012_devkit_t12.tar.gz\n",
    "    !wget -P imagenet https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_val.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfefae6",
   "metadata": {},
   "source": [
    "Extract ImageNet validation dataset using torchvision \"datasets\" module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4ccaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageNet(root='./imagenet', split='val', transform=weights.transforms())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7b3369",
   "metadata": {},
   "source": [
    "## Representative Dataset\n",
    "For quantization with MCT, we need to define a representative dataset required by the PTQ algorithm. This dataset is a generator that returns a list of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62009b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "n_iter = 10\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "def representative_dataset_gen():\n",
    "    dataloader_iter = iter(dataloader)\n",
    "    for _ in range(n_iter):\n",
    "        yield [next(dataloader_iter)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d31427",
   "metadata": {},
   "source": [
    "## Model Evaluation Function\n",
    "Define a comprehensive evaluation function for PyTorch models that provides accurate performance measurement on the validation dataset with GPU acceleration support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956978a3-8817-46f0-8a2c-11c25a6fc063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: torch.nn.Module, testloader: DataLoader, mode: str) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate PyTorch model accuracy using a DataLoader with GPU acceleration.\n",
    "    \n",
    "    This function performs complete accuracy evaluation by:\n",
    "    - Moving model and data to available device (GPU/CPU)\n",
    "    - Running inference in evaluation mode (no gradient computation)\n",
    "    - Computing Top-1 accuracy across the entire validation set\n",
    "    - Providing progress tracking during evaluation\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model to evaluate (float or quantized)\n",
    "        testloader: DataLoader containing validation dataset\n",
    "        mode: String identifier for logging (e.g., 'Float', 'PTQ_Pytorch')\n",
    "    \n",
    "    Returns:\n",
    "        float: Top-1 accuracy percentage\n",
    "    \"\"\"\n",
    "    # Determine best available device for inference\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Perform inference without gradient computation for efficiency\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(testloader):\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass to get predictions\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    # Calculate and display accuracy\n",
    "    val_acc = (100 * correct / total)\n",
    "    print(mode + ' Accuracy: %.2f%%' % val_acc)\n",
    "    return val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea0ee68",
   "metadata": {},
   "source": [
    "## Model Post-Training quantization using MCTWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcb3b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decorator to provide consistent logging and error handling for quantization functions\n",
    "def decorator(func: Callable[[torch.nn.Module], Tuple[bool, torch.nn.Module]]) -> Callable[[torch.nn.Module], Tuple[bool, torch.nn.Module]]:\n",
    "\n",
    "    \"\"\"\n",
    "    Wrapper decorator that provides:\n",
    "    - Consistent start/end logging for quantization operations\n",
    "    - Automatic error handling and program termination on failure\n",
    "    - Success/failure status tracking for all quantization methods\n",
    "    \n",
    "    Args:\n",
    "        func: Quantization function to be decorated\n",
    "    \n",
    "    Returns:\n",
    "        Wrapped function with enhanced logging and error handling\n",
    "    \"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        print(f\"----------------- {func.__name__} Start ---------------\")\n",
    "        flag, result = func(*args, **kwargs)\n",
    "        print(f\"----------------- {func.__name__} End -----------------\")\n",
    "        if not flag:\n",
    "            exit()\n",
    "        return flag, result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e556d1d9",
   "metadata": {
    "cell_marker": "#########################################################################",
    "lines_to_next_cell": 0
   },
   "source": [
    "Run PTQ (Post-Training Quantization) with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe46712",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@decorator\n",
    "def PTQ_Pytorch(float_model: torch.nn.Module) -> Tuple[bool, torch.nn.Module]:\n",
    "    \"\"\"\n",
    "    Perform Post-Training Quantization (PTQ) on PyTorch model.\n",
    "    \n",
    "    PTQ for PyTorch provides:\n",
    "    - Fast quantization without model retraining\n",
    "    - Standard 8-bit integer quantization\n",
    "    - Efficient calibration using representative data\n",
    "    - Direct ONNX export for deployment\n",
    "    \n",
    "    Args:\n",
    "        float_model: Original floating-point PyTorch model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success_flag, quantized_model)\n",
    "    \"\"\"\n",
    "    # Configuration for PyTorch PTQ quantization\n",
    "    framework = 'pytorch'             # Target framework (PyTorch)\n",
    "    method = 'PTQ'                    # Post-Training Quantization method\n",
    "    use_mixed_precision = False                  # Disable mixed-precision quantization\n",
    "\n",
    "    # Parameter configuration for PyTorch PTQ\n",
    "    param_items = [\n",
    "        ['sdsp_version', '3.14'],  # The version of the SDSP converter.\n",
    "        ['activation_error_method', QuantizationErrorMethod.MSE],  # Error metric for activation.\n",
    "        ['weights_bias_correction', True],  # Enable bias correction for weights\n",
    "        ['z_threshold', float('inf')],  # Z-threshold for quantization\n",
    "        ['linear_collapsing', True],  # Enable linear layer collapsing optimization\n",
    "        ['residual_collapsing', True],  # Enable residual layer collapsing optimization\n",
    "        ['save_model_path', './qmodel_PTQ_Pytorch.onnx']  # Path to save quantized model as ONNX.\n",
    "    ]\n",
    "\n",
    "    # Execute PyTorch PTQ quantization using MCTWrapper\n",
    "    wrapper = mct.wrapper.mct_wrapper.MCTWrapper()\n",
    "    flag, quantized_model = wrapper.quantize_and_export(\n",
    "        float_model=float_model, \n",
    "        representative_dataset=representative_dataset_gen, \n",
    "        framework=framework, \n",
    "        method=method, \n",
    "        use_mixed_precision=use_mixed_precision, \n",
    "        param_items=param_items)\n",
    "    return flag, quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6df571",
   "metadata": {
    "cell_marker": "#########################################################################",
    "lines_to_next_cell": 0
   },
   "source": [
    "Run PTQ + Mixed Precision Quantization with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8227d00b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@decorator\n",
    "def PTQ_Pytorch_mixed_precision(float_model: torch.nn.Module) -> Tuple[bool, torch.nn.Module]:\n",
    "    \"\"\"\n",
    "    Perform Post-Training Quantization with Mixed Precision (PTQ + mixed_precision) on PyTorch model.\n",
    "    \n",
    "    Mixed Precision PTQ for PyTorch offers:\n",
    "    - Automatic bit-width selection per layer\n",
    "    - Optimal size/accuracy trade-off\n",
    "    - Resource-constrained quantization\n",
    "    - Advanced sensitivity analysis for PyTorch models\n",
    "    \n",
    "    Args:\n",
    "        float_model: Original floating-point PyTorch model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success_flag, quantized_model)\n",
    "    \"\"\"\n",
    "    # Configuration for PyTorch PTQ with mixed precision\n",
    "    framework = 'pytorch'             # Target framework (PyTorch)\n",
    "    method = 'PTQ'                    # Post-Training Quantization method\n",
    "    use_mixed_precision = True                   # Enable mixed-precision quantization\n",
    "\n",
    "    # Parameter configuration for PyTorch PTQ with Mixed Precision\n",
    "    param_items = [\n",
    "        ['sdsp_version', '3.14'],  # The version of the SDSP converter.\n",
    "        ['num_of_images', 5],  # Number of images for calibration\n",
    "        ['use_hessian_based_scores', False],  # Use Hessian-based sensitivity scores\n",
    "        ['weights_compression_ratio', 0.5],  # Compression ratio for weights\n",
    "        ['save_model_path', './qmodel_PTQ_Pytorch_mixed_precision.onnx']  # Path to save quantized model as ONNX.\n",
    "    ]\n",
    "\n",
    "    # Execute PyTorch mixed precision PTQ using MCTWrapper\n",
    "    wrapper = mct.wrapper.mct_wrapper.MCTWrapper()\n",
    "    flag, quantized_model = wrapper.quantize_and_export(\n",
    "        float_model=float_model, \n",
    "        representative_dataset=representative_dataset_gen, \n",
    "        framework=framework, \n",
    "        method=method, \n",
    "        use_mixed_precision=use_mixed_precision, \n",
    "        param_items=param_items)\n",
    "    return flag, quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04d6a40",
   "metadata": {
    "cell_marker": "#########################################################################",
    "lines_to_next_cell": 0
   },
   "source": [
    "Run GPTQ (Gradient-based PTQ) with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c9da67",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@decorator\n",
    "def GPTQ_Pytorch(float_model: torch.nn.Module) -> Tuple[bool, torch.nn.Module]:\n",
    "    \"\"\"\n",
    "    Perform Gradient-based Post-Training Quantization (GPTQ) on PyTorch model.\n",
    "    \n",
    "    GPTQ for PyTorch provides:\n",
    "    - Advanced gradient-based quantization optimization\n",
    "    - Fine-tuning during quantization process\n",
    "    - Superior accuracy preservation compared to PTQ\n",
    "    - Optimized parameter updates using representative data\n",
    "    \n",
    "    Args:\n",
    "        float_model: Original floating-point PyTorch model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success_flag, quantized_model)\n",
    "    \"\"\"\n",
    "    # Configuration for PyTorch GPTQ quantization\n",
    "    framework = 'pytorch'             # Target framework (PyTorch)\n",
    "    method = 'GPTQ'                   # Gradient-based Post-Training Quantization\n",
    "    use_mixed_precision = False                  # Disable mixed-precision quantization\n",
    "\n",
    "    # Parameter configuration for PyTorch GPTQ\n",
    "    param_items = [\n",
    "        ['sdsp_version', '3.14'],  # The version of the SDSP converter.\n",
    "        ['n_epochs', 5],  # Number of epochs for gradient fine-tuning\n",
    "        ['optimizer', None],  # Optimizer (None = use default)\n",
    "        ['save_model_path', './qmodel_GPTQ_Pytorch.onnx']  # Path to save quantized model as ONNX.\n",
    "    ]\n",
    "\n",
    "    # Execute PyTorch GPTQ quantization using MCTWrapper\n",
    "    wrapper = mct.wrapper.mct_wrapper.MCTWrapper()\n",
    "    flag, quantized_model = wrapper.quantize_and_export(\n",
    "        float_model=float_model, \n",
    "        representative_dataset=representative_dataset_gen, \n",
    "        framework=framework, \n",
    "        method=method, \n",
    "        use_mixed_precision=use_mixed_precision, \n",
    "        param_items=param_items)\n",
    "    return flag, quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e757e27",
   "metadata": {
    "cell_marker": "#########################################################################",
    "lines_to_next_cell": 0
   },
   "source": [
    "Run GPTQ + Mixed Precision Quantization with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2005ef83",
   "metadata": {},
   "outputs": [],
   "source": [
    "@decorator\n",
    "def GPTQ_Pytorch_mixed_precision(float_model: torch.nn.Module) -> Tuple[bool, torch.nn.Module]:\n",
    "    \"\"\"\n",
    "    Perform Gradient-based Post-Training Quantization with Mixed Precision (GPTQ + mixed_precision).\n",
    "    \n",
    "    This advanced method combines:\n",
    "    - GPTQ: Gradient-based optimization for optimal quantization parameters\n",
    "    - Mixed Precision: Automatic bit-width selection for each layer\n",
    "    \n",
    "    Provides the best quantization results for PyTorch models with:\n",
    "    - Maximum accuracy preservation\n",
    "    - Optimal model size reduction\n",
    "    - Layer-wise precision optimization\n",
    "    - Advanced gradient-based calibration\n",
    "    \n",
    "    Args:\n",
    "        float_model: Original floating-point PyTorch model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success_flag, quantized_model)\n",
    "    \"\"\"\n",
    "    # Configuration for PyTorch GPTQ with mixed precision\n",
    "    framework = 'pytorch'             # Target framework (PyTorch)\n",
    "    method = 'GPTQ'                   # Gradient-based Post-Training Quantization\n",
    "    use_mixed_precision = True                   # Enable mixed-precision quantization\n",
    "\n",
    "    # Parameter configuration for PyTorch GPTQ with Mixed Precision\n",
    "    param_items = [\n",
    "        ['sdsp_version', '3.14'],  # The version of the SDSP converter.\n",
    "        ['n_epochs', 5],  # Number of epochs for gradient fine-tuning\n",
    "        ['optimizer', None],  # Optimizer (None = use default)\n",
    "        ['num_of_images', 5],  # Number of images for calibration\n",
    "        ['use_hessian_based_scores', False],  # Use Hessian-based sensitivity scores\n",
    "        ['weights_compression_ratio', 0.5],  # Compression ratio for weights\n",
    "        ['save_model_path', './qmodel_GPTQ_Pytorch_mixed_precision.onnx']  # Path to save quantized model as ONNX.\n",
    "    ]\n",
    "\n",
    "    # Execute advanced PyTorch GPTQ+mixed_precision quantization using MCTWrapper\n",
    "    wrapper = mct.wrapper.mct_wrapper.MCTWrapper()\n",
    "    flag, quantized_model = wrapper.quantize_and_export(\n",
    "        float_model=float_model, \n",
    "        representative_dataset=representative_dataset_gen, \n",
    "        framework=framework, \n",
    "        method=method, \n",
    "        use_mixed_precision=use_mixed_precision, \n",
    "        param_items=param_items)\n",
    "    return flag, quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07704042",
   "metadata": {},
   "source": [
    "### Run model Post-Training Quantization\n",
    "Lastly, we quantize our model using MCTWrapper API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eee799-c5e8-4004-b120-fa23d08da74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for validation/evaluation with larger batch size for efficiency\n",
    "val_dataloader = DataLoader(dataset, batch_size=50, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d5d54c-0686-4c81-a24b-d8c1d84deea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute all PyTorch quantization methods on the same base model for comparison\n",
    "print(\"Starting PyTorch quantization experiments with different methods...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b29a22-dbbd-4aac-aba7-d3281324d31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Basic Post-Training Quantization for PyTorch\n",
    "flag, quantized_mode_ptq = PTQ_Pytorch(float_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e47383-d512-47bb-8602-bbc284f3786c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. PTQ with Mixed Precision (optimized size/accuracy trade-off for PyTorch)\n",
    "flag, quantized_model_ptq_mixed_precision = PTQ_Pytorch_mixed_precision(float_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f3cd87-dde5-4463-8e84-1b3b11e447e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Gradient-based PTQ (improved accuracy through fine-tuning for PyTorch)\n",
    "flag, quantized_model_gptq = GPTQ_Pytorch(float_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecb3322-47d4-4fcd-8e6c-77037206d7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. GPTQ with Mixed Precision (best accuracy with optimal compression for PyTorch)\n",
    "flag, quantized_model_gptq_mixed_precision = GPTQ_Pytorch_mixed_precision(float_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969737a3-5181-4331-953c-243ca965f630",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All PyTorch quantization methods completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2172853",
   "metadata": {},
   "source": [
    "## Models evaluation\n",
    "In order to evaluate our models, we first need to load the validation dataset. As before, please ensure that the dataset path has been set correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56da7fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Model Evaluation and Accuracy Comparison\n",
    "print(\"Starting PyTorch model evaluation phase...\")\n",
    "print(\"This evaluation will test all quantized models against the validation dataset\")\n",
    "\n",
    "# Evaluate original floating-point PyTorch model accuracy\n",
    "print(\"\\n=== Original PyTorch Model Evaluation ===\")\n",
    "evaluate(float_model, val_dataloader, 'Float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19492a4c-6ec7-4643-82c0-ce448ba0b8de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate PTQ quantized PyTorch model accuracy\n",
    "print(\"\\n=== PyTorch PTQ Model Evaluation ===\")\n",
    "evaluate(quantized_mode_ptq, val_dataloader, 'PTQ_Pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd1bc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate PTQ + Mixed Precision PyTorch model accuracy\n",
    "print(\"\\n=== PyTorch PTQ + Mixed Precision Model Evaluation ===\")\n",
    "evaluate(quantized_model_ptq_mixed_precision, val_dataloader, 'PTQ_Pytorch_mixed_precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae625b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate GPTQ quantized PyTorch model accuracy\n",
    "print(\"\\n=== PyTorch GPTQ Model Evaluation ===\")\n",
    "evaluate(quantized_model_gptq, val_dataloader, 'GPTQ_Pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c579cb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate GPTQ + Mixed Precision PyTorch model accuracy\n",
    "print(\"\\n=== PyTorch GPTQ + Mixed Precision Model Evaluation ===\")\n",
    "evaluate(quantized_model_gptq_mixed_precision, val_dataloader, 'GPTQ_Pytorch_mixed_precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6521eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"finish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37baa56",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we demonstrated how to quantize a pre-trained model using MCTWrapper with a few lines of code.\n",
    "\n",
    "MCT can deliver competitive results across a wide range of tasks and network architectures. For more details, [check out the paper:](https://arxiv.org/abs/2109.09113).\n",
    "\n",
    "## Copyrights\n",
    "\n",
    "Copyright 2025 Sony Semiconductor Solutions, Inc. All rights reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
