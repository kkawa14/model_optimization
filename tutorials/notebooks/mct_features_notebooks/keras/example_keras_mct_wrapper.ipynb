{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9ca09f9",
   "metadata": {},
   "source": [
    "# Model Compression Toolkit (MCT) Wrapper API Comprehensive Quantization Comparison(tensorflow)\n",
    "\n",
    "[Run this tutorial in Google Colab](https://colab.research.google.com/github/SonySemiconductorSolutions/mct-model-optimization/blob/main/tutorials/notebooks/mct_features_notebooks/keras/example_keras_mct_wrapper.ipynb)\n",
    "\n",
    "## Overview \n",
    "This notebook provides a comprehensive demonstration of the MCT (Model Compression Toolkit) Wrapper API functionality, showcasing five different quantization methods on a MobileNetV2 model. The tutorial systematically compares the implementation, performance characteristics, and accuracy trade-offs of each quantization approach: PTQ (Post-Training Quantization), PTQ with Mixed Precision, GPTQ (Gradient-based PTQ), GPTQ with Mixed Precision. Each method utilizes the unified MCTWrapper interface for consistent implementation and comparison.\n",
    "\n",
    "## Summary\n",
    "1. **Environment Setup**: Import required libraries and configure MCT with MobileNetV2 model\n",
    "2. **Dataset Preparation**: Load and prepare ImageNet validation dataset with representative data generation\n",
    "3. **PTQ Implementation**: Execute basic Post-Training Quantization with 8-bit precision and bias correction\n",
    "4. **PTQ + Mixed Precision**: Apply intelligent bit-width allocation based on layer sensitivity analysis (75% compression ratio)\n",
    "5. **GPTQ Implementation**: Perform gradient-based optimization with 5-epoch fine-tuning for enhanced accuracy\n",
    "6. **GPTQ + Mixed Precision**: Combine gradient optimization with mixed precision for optimal accuracy-compression trade-off\n",
    "7. **Performance Evaluation**: Comprehensive accuracy assessment and comparison across all quantization methods\n",
    "8. **Results Analysis**: Compare model sizes, inference accuracy, and quantization trade-offs\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e667e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_VER = '2.14.0'\n",
    "!pip install -q tensorflow~={TF_VER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e138ad45-68d2-4c34-95ac-1a327ce54406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "if not importlib.util.find_spec('model_compression_toolkit'):\n",
    "    !pip install model_compression_toolkit\n",
    "\n",
    "import model_compression_toolkit as mct\n",
    "from model_compression_toolkit.core import QuantizationErrorMethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0ece2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from typing import Tuple, Callable, Generator, List, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60853205",
   "metadata": {},
   "source": [
    "Load a pre-trained MobileNetV2 model from Keras, in 32-bits floating-point precision format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e743078d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "\n",
    "float_model = MobileNetV2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dce48a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Dataset preparation\n",
    "Download ImageNet dataset with only the validation split.\n",
    "\n",
    "**Note** that for demonstration purposes we use the validation set for the model quantization routines. Usually, a subset of the training dataset is used, but loading it is a heavy procedure that is unnecessary for the sake of this demonstration.\n",
    "\n",
    "This step may take several minutes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f14d92-5116-40b7-9090-f378bdf3cc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    " \n",
    "if not os.path.isdir('imagenet'):\n",
    "    !mkdir imagenet\n",
    "    !wget -P imagenet https://image-net.org/data/ILSVRC/2012/ILSVRC2012_devkit_t12.tar.gz\n",
    "    !wget -P imagenet https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_val.tar\n",
    "    \n",
    "    !cd imagenet && tar -xzf ILSVRC2012_devkit_t12.tar.gz && \\\n",
    "     mkdir ILSVRC2012_img_val && tar -xf ILSVRC2012_img_val.tar -C ILSVRC2012_img_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41859aab",
   "metadata": {},
   "source": [
    "The following code organizes the extracted data into separate folders for each label, making it compatible with Keras dataset loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4bd4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "root = Path('./imagenet')\n",
    "imgs_dir = root / 'ILSVRC2012_img_val'\n",
    "target_dir = root /'val'\n",
    "\n",
    "def extract_labels():\n",
    "    !pip install -q scipy\n",
    "    import scipy\n",
    "    mat = scipy.io.loadmat(root / 'ILSVRC2012_devkit_t12/data/meta.mat', squeeze_me=True)\n",
    "    cls_to_nid = {s[0]: s[1] for i, s in enumerate(mat['synsets']) if s[4] == 0} \n",
    "    with open(root / 'ILSVRC2012_devkit_t12/data/ILSVRC2012_validation_ground_truth.txt', 'r') as f:\n",
    "        return [cls_to_nid[int(cls)] for cls in f.readlines()]\n",
    "\n",
    "if not target_dir.exists():\n",
    "    labels = extract_labels()\n",
    "    for lbl in set(labels):\n",
    "        os.makedirs(target_dir / lbl)\n",
    "    \n",
    "    for img_file, lbl in zip(sorted(os.listdir(imgs_dir)), labels):\n",
    "        shutil.move(imgs_dir / img_file, target_dir / lbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8dd14b",
   "metadata": {},
   "source": [
    "These functions generate a `tf.data.Dataset` from image files in a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5212d5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imagenet_preprocess_input(images: tf.Tensor, labels: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "    return tf.keras.applications.mobilenet_v2.preprocess_input(images), labels\n",
    "\n",
    "def get_dataset(batch_size: int, shuffle: bool) -> tf.data.Dataset:\n",
    "    dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        directory='./imagenet/val',\n",
    "        batch_size=batch_size,\n",
    "        image_size=[224, 224],\n",
    "        shuffle=shuffle,\n",
    "        crop_to_aspect_ratio=True,\n",
    "        interpolation='bilinear')\n",
    "    dataset = dataset.map(lambda x, y: (imagenet_preprocess_input(x, y)), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c490a2e",
   "metadata": {},
   "source": [
    "## Representative Dataset\n",
    "For quantization with MCT, we need to define a representative dataset required by the PTQ algorithm. This dataset is a generator that returns a list of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f60a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "n_iter = 5\n",
    "\n",
    "dataset = get_dataset(batch_size, shuffle=True)\n",
    "\n",
    "def representative_dataset_gen() -> Generator[List[Any], None, None]:\n",
    "    for _ in range(n_iter):\n",
    "        yield [dataset.take(1).get_single_element()[0].numpy()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa8e319",
   "metadata": {},
   "source": [
    "## Model Post-Training quantization using MCTWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd8db92-9151-42d6-b3fa-311f7d528df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decorator to provide consistent logging and error handling for quantization functions\n",
    "def decorator(func: Callable[[keras.Model], Tuple[bool, keras.Model]]) -> Callable[[keras.Model], Tuple[bool, keras.Model]]:\n",
    "    \"\"\"\n",
    "    Wrapper decorator that provides standardized execution logging and error handling.\n",
    "    \n",
    "    This decorator enhances quantization functions by:\n",
    "    - Providing clear start/end execution markers for debugging\n",
    "    - Handling success/failure status from quantization operations\n",
    "    - Implementing fail-fast behavior on quantization errors\n",
    "    - Ensuring consistent logging format across all quantization methods\n",
    "    \n",
    "    Usage:\n",
    "        @decorator\n",
    "        def quantization_function(model):\n",
    "            # quantization implementation\n",
    "            return flag, quantized_model\n",
    "    \n",
    "    Args:\n",
    "        func: Function to be decorated (typically a quantization function)\n",
    "    \n",
    "    Returns:\n",
    "        Wrapped function with enhanced logging and error handling capabilities\n",
    "    \"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # Log function execution start with clear delimiter\n",
    "        print(f\"----------------- {func.__name__} Start ---------------\")\n",
    "        \n",
    "        # Execute the quantization function and capture return values\n",
    "        # Expected return format: (success_flag, quantized_model)\n",
    "        flag, result = func(*args, **kwargs)\n",
    "        \n",
    "        # Log function execution completion\n",
    "        print(f\"----------------- {func.__name__} End -----------------\")\n",
    "        \n",
    "        # Implement fail-fast behavior: exit immediately on quantization failure\n",
    "        # This ensures early detection of quantization issues\n",
    "        if not flag:\n",
    "            exit()\n",
    "        \n",
    "        # Return original function results if successful\n",
    "        return flag, result\n",
    "    \n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf35f64",
   "metadata": {
    "cell_marker": "#########################################################################",
    "lines_to_next_cell": 0
   },
   "source": [
    "Run PTQ (Post-Training Quantization) with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e151ddca",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@decorator\n",
    "def PTQ_Keras(float_model: keras.Model) -> Tuple[bool, keras.Model]:\n",
    "    \"\"\"\n",
    "    Perform Post-Training Quantization (PTQ) using MCT on Keras model.\n",
    "    \n",
    "    PTQ is a quantization method that:\n",
    "    - Does not require model retraining\n",
    "    - Uses representative data for calibration\n",
    "    - Provides good accuracy with minimal computational overhead\n",
    "    \n",
    "    Args:\n",
    "        float_model: Original floating-point Keras model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success_flag, quantized_model)\n",
    "    \"\"\"\n",
    "    # Configuration for basic PTQ quantization\n",
    "    method = 'PTQ'                    # Post-Training Quantization method\n",
    "    framework = 'tensorflow'          # Target framework (Keras/TensorFlow)\n",
    "    use_internal_tpc = True                # Use MCT's built-in Target Platform Capabilities\n",
    "    use_mixed_precision = False                  # Disable mixed-precision quantization\n",
    "\n",
    "    # Parameter configuration for PTQ\n",
    "    param_items = [\n",
    "        ['target_platform_version', 'v1'],  # The version of the TPC to use.\n",
    "        ['activation_error_method', QuantizationErrorMethod.MSE],  # Error metric for activation.\n",
    "        ['weights_bias_correction', True],  # Enable bias correction for weights.\n",
    "        ['z_threshold', float('inf')],  # Threshold for zero-point quantization.\n",
    "        ['linear_collapsing', True],  # Enable linear layer collapsing optimization.\n",
    "        ['residual_collapsing', True],  # Enable residual connection collapsing.\n",
    "        ['save_model_path', './qmodel_PTQ_Keras.keras']  # Path to save the quantized model.\n",
    "    ]\n",
    "\n",
    "    # Execute quantization using MCTWrapper\n",
    "    wrapper = mct.wrapper.mct_wrapper.MCTWrapper()\n",
    "    flag, quantized_model = wrapper.quantize_and_export(\n",
    "        float_model=float_model, \n",
    "        representative_dataset=representative_dataset_gen,\n",
    "        method=method, \n",
    "        framework=framework, \n",
    "        use_internal_tpc=use_internal_tpc, \n",
    "        use_mixed_precision=use_mixed_precision, \n",
    "        param_items=param_items)\n",
    "    return flag, quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7f6eeb",
   "metadata": {
    "cell_marker": "#########################################################################",
    "lines_to_next_cell": 0
   },
   "source": [
    "Run PTQ + Mixed Precision Quantization with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c082471a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@decorator\n",
    "def PTQ_Keras_mixed_precision(float_model: keras.Model) -> Tuple[bool, keras.Model]:\n",
    "    \"\"\"\n",
    "    Perform Post-Training Quantization with Mixed Precision (PTQ + mixed_precision) on Keras model.\n",
    "    \n",
    "    Mixed Precision Quantization:\n",
    "    - Uses different bit-widths for different layers\n",
    "    - Optimizes model size while maintaining accuracy\n",
    "    - Automatically selects optimal precision for each layer\n",
    "    - Uses resource constraints to guide precision allocation\n",
    "    \n",
    "    Args:\n",
    "        float_model: Original floating-point Keras model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success_flag, quantized_model)\n",
    "    \"\"\"\n",
    "    # Configuration for PTQ with mixed precision\n",
    "    method = 'PTQ'                    # Post-Training Quantization method\n",
    "    framework = 'tensorflow'          # Target framework (Keras/TensorFlow)\n",
    "    use_internal_tpc = True                # Use MCT's built-in Target Platform Capabilities\n",
    "    use_mixed_precision = True                   # Enable mixed-precision quantization\n",
    "\n",
    "    # Parameter configuration for PTQ with Mixed Precision\n",
    "    param_items = [\n",
    "        ['target_platform_version', 'v1'],  # The version of the TPC to use.\n",
    "        ['num_of_images', 5],  # Number of epochs for gradient-based fine-tuning.\n",
    "        ['use_hessian_based_scores', False],  # Use Hessian-based sensitivity scores for layer importance.\n",
    "        ['weights_compression_ratio', 0.75],  # Target compression ratio for model weights (75% of original size.\n",
    "        ['save_model_path', './qmodel_PTQ_Keras_mixed_precision.keras']  # Path to save the quantized model.\n",
    "    ]\n",
    "\n",
    "    # Execute mixed precision quantization using MCTWrapper\n",
    "    wrapper = mct.wrapper.mct_wrapper.MCTWrapper()\n",
    "    flag, quantized_model = wrapper.quantize_and_export(\n",
    "        float_model=float_model, \n",
    "        representative_dataset=representative_dataset_gen,\n",
    "        method=method, \n",
    "        framework=framework, \n",
    "        use_internal_tpc=use_internal_tpc, \n",
    "        use_mixed_precision=use_mixed_precision, \n",
    "        param_items=param_items)\n",
    "    return flag, quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269f2cba",
   "metadata": {
    "cell_marker": "#########################################################################",
    "lines_to_next_cell": 0
   },
   "source": [
    "Run GPTQ (Gradient-based PTQ) with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82c0070",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@decorator\n",
    "def GPTQ_Keras(float_model: keras.Model) -> Tuple[bool, keras.Model]:\n",
    "    \"\"\"\n",
    "    Perform Gradient-based Post-Training Quantization (GPTQ) on Keras model.\n",
    "    \n",
    "    GPTQ is an advanced quantization method that:\n",
    "    - Uses gradient information to optimize quantization parameters\n",
    "    - Fine-tunes the model during quantization process\n",
    "    - Generally provides better accuracy than standard PTQ\n",
    "    - Requires slightly more computational resources than PTQ\n",
    "    \n",
    "    Args:\n",
    "        float_model: Original floating-point Keras model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success_flag, quantized_model)\n",
    "    \"\"\"\n",
    "    # Configuration for GPTQ quantization\n",
    "    method = 'GPTQ'                   # Gradient-based Post-Training Quantization\n",
    "    framework = 'tensorflow'          # Target framework (Keras/TensorFlow)\n",
    "    use_internal_tpc = True                # Use MCT's built-in Target Platform Capabilities\n",
    "    use_mixed_precision = False                  # Disable mixed-precision quantization\n",
    "\n",
    "    # Parameter configuration for GPTQ\n",
    "    param_items = [\n",
    "        # Platform configuration\n",
    "        ['target_platform_version', 'v1'],  # The version of the TPC to use.\n",
    "        ['n_epochs', 5],  # Number of epochs for gradient-based fine-tuning.\n",
    "        ['optimizer', None],  # Optimizer to use during fine-tuning.\n",
    "        ['save_model_path', './qmodel_GPTQ_Keras.keras']  # Path to save the quantized model.\n",
    "    ]\n",
    "\n",
    "    # Execute GPTQ quantization using MCTWrapper\n",
    "    wrapper = mct.wrapper.mct_wrapper.MCTWrapper()\n",
    "    flag, quantized_model = wrapper.quantize_and_export(\n",
    "        float_model=float_model, \n",
    "        representative_dataset=representative_dataset_gen,\n",
    "        method=method, \n",
    "        framework=framework, \n",
    "        use_internal_tpc=use_internal_tpc, \n",
    "        use_mixed_precision=use_mixed_precision, \n",
    "        param_items=param_items)\n",
    "    return flag, quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9d77b5",
   "metadata": {
    "cell_marker": "#########################################################################",
    "lines_to_next_cell": 0
   },
   "source": [
    "Run GPTQ + Mixed Precision Quantization with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e06b4e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@decorator\n",
    "def GPTQ_Keras_mixed_precision(float_model: keras.Model) -> Tuple[bool, keras.Model]:\n",
    "    \"\"\"\n",
    "    Perform Gradient-based Post-Training Quantization with Mixed Precision (GPTQ + mixed_precision).\n",
    "    \n",
    "    This combines the benefits of both techniques:\n",
    "    - GPTQ: Gradient-based optimization for better quantization accuracy\n",
    "    - Mixed Precision: Optimal bit-width allocation for size/accuracy trade-off\n",
    "    \n",
    "    This is the most advanced quantization method available, providing:\n",
    "    - Best possible accuracy preservation\n",
    "    - Optimal model size reduction\n",
    "    - Automatic precision selection per layer\n",
    "    \n",
    "    Args:\n",
    "        float_model: Original floating-point Keras model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success_flag, quantized_model)\n",
    "    \"\"\"\n",
    "    # Configuration for GPTQ with mixed precision\n",
    "    method = 'GPTQ'                   # Gradient-based Post-Training Quantization\n",
    "    framework = 'tensorflow'          # Target framework (Keras/TensorFlow)\n",
    "    use_internal_tpc = True                # Use MCT's built-in Target Platform Capabilities\n",
    "    use_mixed_precision = True                   # Enable mixed-precision quantization\n",
    "\n",
    "    # Parameter configuration for GPTQ with Mixed Precision\n",
    "    param_items = [\n",
    "        # Platform configuration\n",
    "        ['target_platform_version', 'v1'],  # The version of the TPC to use.\n",
    "        ['n_epochs', 5],  # Number of epochs for gradient-based fine-tuning.\n",
    "        ['optimizer', None],  # Optimizer to use during fine-tuning.\n",
    "        ['num_of_images', 5],  # Number of images to use for calibration.\n",
    "        ['use_hessian_based_scores', False],  # Whether to use Hessian-based scores for layer importance.\n",
    "        ['weights_compression_ratio', 0.75],  # Compression ratio for weights.\n",
    "        ['save_model_path', './qmodel_GPTQ_Keras_mixed_precision.keras']  # Path to save the quantized model.\n",
    "    ]\n",
    "\n",
    "    # Execute advanced GPTQ with mixed precision using MCTWrapper\n",
    "    wrapper = mct.wrapper.mct_wrapper.MCTWrapper()\n",
    "    flag, quantized_model = wrapper.quantize_and_export(\n",
    "        float_model=float_model, \n",
    "        representative_dataset=representative_dataset_gen,\n",
    "        method=method, \n",
    "        framework=framework, \n",
    "        use_internal_tpc=use_internal_tpc, \n",
    "        use_mixed_precision=use_mixed_precision, \n",
    "        param_items=param_items)\n",
    "    return flag, quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314fc4ce",
   "metadata": {},
   "source": [
    "### Run model Post-Training Quantization\n",
    "Lastly, we quantize our model using MCTWrapper API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b959792-f339-43a9-9573-a95052b28e88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Execute comprehensive quantization method comparison using MCT Wrapper functionality\n",
    "# Each method represents different trade-offs between accuracy, model size, and computation time\n",
    "print(\"Starting quantization experiments with different methods...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce5ebd0-aa83-4bb7-9e23-5ae3b95858dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Basic Post-Training Quantization (PTQ)\n",
    "# - Standard 8-bit quantization without advanced optimization techniquesed\n",
    "flag, quantized_model_ptq = PTQ_Keras(float_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9990c4d4-0ffe-4069-ba9b-6d14e9722edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PTQ with Mixed Precision Quantization\n",
    "# - Uses different bit-widths for different layers based on sensitivity analysis\n",
    "flag, quantized_model_ptq_mixed_precision = PTQ_Keras_mixed_precision(float_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b952197-11c2-4373-842e-ce3b264ca5b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Gradient-based Post-Training Quantization (GPTQ)\n",
    "# - Uses gradient information to fine-tune quantization parameters during conversion\n",
    "flag, quantized_model_gptq = GPTQ_Keras(float_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075a2012-9423-4eaa-84c7-365a5daafa6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GPTQ with Mixed Precision Quantization\n",
    "# - Combines gradient-based optimization with mixed precision techniques\n",
    "flag, quantized_model_gptq_mixed_precision = GPTQ_Keras_mixed_precision(float_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19453943-75ba-4707-bef9-e137ccb91094",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All quantization methods completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b34232",
   "metadata": {},
   "source": [
    "## Models evaluation\n",
    "In order to evaluate our models, we first need to load the validation dataset. As before, please ensure that the dataset path has been set correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce90826-e5c5-4be0-8b54-96795eab47eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation and Accuracy Comparison\n",
    "print(\"Starting model evaluation phase...\")\n",
    "\n",
    "# Prepare validation dataset for accuracy assessment\n",
    "val_dataset = get_dataset(batch_size=50, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e01931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate original floating-point model accuracy\n",
    "print(\"\\n=== Original Model Evaluation ===\")\n",
    "float_model.compile(loss=keras.losses.SparseCategoricalCrossentropy(), metrics=\"accuracy\")\n",
    "float_accuracy = float_model.evaluate(val_dataset)\n",
    "print(f\"Float model's Top 1 accuracy on the Imagenet validation set: {(float_accuracy[1] * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cabfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate PTQ quantized model accuracy\n",
    "print(\"\\n=== PTQ Model Evaluation ===\")\n",
    "quantized_model_ptq.compile(loss=keras.losses.SparseCategoricalCrossentropy(), metrics=\"accuracy\")\n",
    "quantized_accuracy = quantized_model_ptq.evaluate(val_dataset)\n",
    "print(f\"PTQ_Keras Quantized model's Top 1 accuracy on the Imagenet validation set: {(quantized_accuracy[1] * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a42b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate PTQ + Mixed Precision model accuracy\n",
    "print(\"\\n=== PTQ + Mixed Precision Model Evaluation ===\")\n",
    "quantized_model_ptq_mixed_precision.compile(loss=keras.losses.SparseCategoricalCrossentropy(), metrics=\"accuracy\")\n",
    "quantized_accuracy = quantized_model_ptq_mixed_precision.evaluate(val_dataset)\n",
    "print(f\"PTQ_Keras_mixed_precision Quantized model's Top 1 accuracy on the Imagenet validation set: {(quantized_accuracy[1] * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad8438d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate GPTQ quantized model accuracy\n",
    "print(\"\\n=== GPTQ Model Evaluation ===\")\n",
    "quantized_model_gptq.compile(loss=keras.losses.SparseCategoricalCrossentropy(), metrics=\"accuracy\")\n",
    "quantized_accuracy = quantized_model_gptq.evaluate(val_dataset)\n",
    "print(f\"GPTQ_Keras Quantized model's Top 1 accuracy on the Imagenet validation set: {(quantized_accuracy[1] * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46335127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate GPTQ + Mixed Precision model accuracy\n",
    "print(\"\\n=== GPTQ + Mixed Precision Model Evaluation ===\")\n",
    "quantized_model_gptq_mixed_precision.compile(loss=keras.losses.SparseCategoricalCrossentropy(), metrics=\"accuracy\")\n",
    "quantized_accuracy = quantized_model_gptq_mixed_precision.evaluate(val_dataset)\n",
    "print(f\"GPTQ_Keras_mixed_precision Quantized model's Top 1 accuracy on the Imagenet validation set: {(quantized_accuracy[1] * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaf122d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"finish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e71f63d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we demonstrated how to quantize a pre-trained model using MCTWrapper with a few lines of code.\n",
    "\n",
    "MCT can deliver competitive results across a wide range of tasks and network architectures. For more details, [check out the paper:](https://arxiv.org/abs/2109.09113).\n",
    "\n",
    "## Copyrights\n",
    "\n",
    "Copyright 2025 Sony Semiconductor Solutions, Inc. All rights reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
